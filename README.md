# Final Project - Dashboard

The main idea is to visualize the performance of different Large Language Models (LLMs)
on code generation tasks using the **EvoCodeBench dataset**. The dashboard will allow you
to explore the performance of LLMs in different contexts and across multiple repositories.
The project will focus on evaluating key metrics such as **Pass@k** (functional correctness)
and **Recall@k** (reference dependency recall).

The dataset can be obtained from: https://huggingface.co/datasets/LJ0815/EvoCodeBench.

**Layer 1: Parameters Menu**
- The dataset primarily provides information about the repositories used, as it is a
benchmark. However, we can access the project at:
https://github.com/seketeam/EvoCodeBench.

The project contains information about the code generated by each model, so only
the metrics are needed to run them. Therefore, this dashboard have the following filters:

- **Template:** Baseline, Local Completion, Local Infilling and kg.
- **Plot type:** Scatter Plot, Heatmap
- **X-axis (pass@k metrics):** Pass@1, Pass@3, Pass@5, Pass@10
- **Y-axis (recall@k metrics):** Recall@1, Recall@3, Recall@5, Recall@10

**Layer 2: Graphical Representation of Source Data**
- To display the data, I used a heat map to show the performance of each
model across different templates and also you can select a scatter plot to
compare two metrics for different models in different templates.

**Layer 3: Metrics**
- Different metrics will be used for the evaluation, including Pass@k for Functional
Correctness and Recall@k for Recall of Reference Dependency, so each of them
can be graphed according to the results generated by each model. So, the
dashboard will dynamically generate graphs based on the selected metrics.
